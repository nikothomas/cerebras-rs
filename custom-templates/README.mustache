# Cerebras Rust SDK

[![Crates.io](https://img.shields.io/crates/v/cerebras-rs.svg)](https://crates.io/crates/cerebras-rs)
[![Documentation](https://docs.rs/cerebras-rs/badge.svg)](https://docs.rs/cerebras-rs)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

High-performance Rust SDK for the Cerebras Inference API, providing low-latency AI model inference powered by Cerebras Wafer-Scale Engines and CS-3 systems.

## Features

- ðŸš€ **High Performance** - Optimized for Cerebras' ultra-fast inference
- ðŸ”„ **Async/Await** - Built on Tokio for concurrent operations
- ðŸ“¡ **Streaming Support** - Real-time token streaming for chat and completions
- ðŸ› ï¸ **Builder Patterns** - Ergonomic API for constructing requests
- ðŸŽ¯ **Type Safety** - Strongly typed requests and responses
- ðŸ”§ **Function Calling** - Full support for tool use and function calling
- ðŸ“ **Comprehensive Examples** - Ready-to-run examples for common use cases

## Installation

Add this to your `Cargo.toml`:

```toml
[dependencies]
cerebras-rs = "{{packageVersion}}"

# For streaming support
cerebras-rs = { version = "{{packageVersion}}", features = ["stream"] }
```

## Quick Start

```rust
use cerebras_rs::prelude::*;

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    // Create client from environment variable CEREBRAS_API_KEY
    let client = Client::from_env()?;
    
    // Create a chat completion
    let request = ChatCompletionRequest::builder(ModelIdentifier::Llama3_1_8b)
        .system_message("You are a helpful assistant")
        .user_message("What is the capital of France?")
        .temperature(0.7)
        .build();
    
    let response = client.chat_completion(request).await?;
    println!("{}", response.choices[0].message.content);
    
    Ok(())
}
```

## Authentication

Set your API key as an environment variable:

```bash
export CEREBRAS_API_KEY="your-api-key-here"
```

Or create a client with an explicit API key:

```rust
let client = Client::new("your-api-key-here");
```

## Available Models

The SDK supports all Cerebras models:

- `ModelIdentifier::Llama4Scout17b16eInstruct` - Llama 4 Scout 17B
- `ModelIdentifier::Llama3_1_8b` - Llama 3.1 8B
- `ModelIdentifier::Llama3_3_70b` - Llama 3.3 70B
- `ModelIdentifier::Qwen3_32b` - Qwen 3 32B
- `ModelIdentifier::DeepseekR1DistillLlama70b` - Deepseek R1 Distill Llama 70B

## Examples

### Chat Completion with Builder Pattern

```rust
use cerebras_rs::prelude::*;

let request = ChatCompletionRequest::builder(ModelIdentifier::Llama3_1_8b)
    .system_message("You are a helpful math tutor")
    .user_message("What is 15 + 27?")
    .temperature(0.3)
    .max_tokens(100)
    .build();

let response = client.chat_completion(request).await?;
```

### Streaming Responses

```rust
use cerebras_rs::prelude::*;
use futures_util::StreamExt;

let request = ChatCompletionRequest::builder(ModelIdentifier::Llama3_1_8b)
    .user_message("Tell me a story")
    .stream(true)
    .build();

let mut stream = client.chat_completion_stream(request).await?;

while let Some(chunk) = stream.next().await {
    match chunk {
        Ok(chunk) => {
            if let Some(content) = &chunk.choices[0].delta.content {
                print!("{}", content);
            }
        }
        Err(e) => eprintln!("Error: {}", e),
    }
}
```

### Function Calling

```rust
use cerebras_rs::prelude::*;
use cerebras_rs::models::{Tool, FunctionDefinition};
use serde_json::json;

let weather_tool = Tool {
    r#type: "function".to_string(),
    function: FunctionDefinition {
        name: "get_weather".to_string(),
        description: Some("Get current weather".to_string()),
        parameters: Some(json!({
            "type": "object",
            "properties": {
                "location": {
                    "type": "string",
                    "description": "City and state"
                }
            },
            "required": ["location"]
        })),
    },
};

let request = ChatCompletionRequest::builder(ModelIdentifier::Llama3_1_8b)
    .user_message("What's the weather in New York?")
    .tool(weather_tool)
    .build();

let response = client.chat_completion(request).await?;

if let Some(tool_calls) = &response.choices[0].message.tool_calls {
    for call in tool_calls {
        println!("Function: {}", call.function.name);
        println!("Arguments: {}", call.function.arguments);
    }
}
```

### Text Completion

```rust
use cerebras_rs::prelude::*;

let request = CompletionRequest::builder(ModelIdentifier::Llama3_1_8b)
    .prompt("Once upon a time")
    .max_tokens(100)
    .temperature(0.8)
    .build();

let response = client.completion(request).await?;
println!("{}", response.choices[0].text);
```

## Error Handling

The SDK provides comprehensive error handling:

```rust
use cerebras_rs::{Client, Error};

match client.chat_completion(request).await {
    Ok(response) => println!("Success: {}", response.choices[0].message.content),
    Err(Error::RateLimit { retry_after }) => {
        println!("Rate limited. Retry after {:?} seconds", retry_after);
    }
    Err(Error::Authentication) => {
        println!("Invalid API key");
    }
    Err(e) => {
        println!("Error: {}", e);
    }
}
```

## Advanced Features

### Custom Configuration

```rust
use cerebras_rs::{Client, Configuration};

let mut config = Configuration::new();
config.base_path = "https://custom-endpoint.example.com".to_string();
config.bearer_access_token = Some("your-api-key".to_string());

let client = Client::with_configuration(config);
```

### Response Metadata

```rust
let response = client.chat_completion(request).await?;

// Token usage
if let Some(usage) = &response.usage {
    println!("Prompt tokens: {}", usage.prompt_tokens);
    println!("Completion tokens: {}", usage.completion_tokens);
    println!("Total tokens: {}", usage.total_tokens);
}

// Timing information
if let Some(time_info) = &response.time_info {
    println!("Queue time: {:.3}s", time_info.queue_time);
    println!("Prompt time: {:.3}s", time_info.prompt_time);
    println!("Completion time: {:.3}s", time_info.completion_time);
    println!("Total time: {:.3}s", time_info.total_time);
}
```

## More Examples

Check out the `examples/` directory for more comprehensive examples:

- `chat_completion.rs` - Various chat completion scenarios
- `streaming.rs` - Streaming response handling
- `function_calling.rs` - Tool use and function calling

Run examples with:

```bash
cargo run --example chat_completion --features stream
cargo run --example streaming --features stream
cargo run --example function_calling --features stream
```

## Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## License

This project is licensed under the MIT License - see the LICENSE file for details.

## Links

- [API Documentation](https://docs.rs/cerebras-rs)
- [Cerebras Inference Docs](https://inference-docs.cerebras.ai)
- [GitHub Repository](https://github.com/cerebras/cerebras-rs)
