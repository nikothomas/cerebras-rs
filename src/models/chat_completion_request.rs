/*
 * Cerebras Inference API
 *
 * The Cerebras Inference API offers developers a low-latency solution for AI model inference  powered by Cerebras Wafer-Scale Engines and CS-3 systems. The API provides access to  high-performance language models with unprecedented speed for AI inference workloads.
 *
 * The version of the OpenAPI document: 1.0.0
 * Contact: support@cerebras.ai
 * Generated by: https://openapi-generator.tech
 */

use crate::models;
use serde::{Deserialize, Serialize};

#[derive(Clone, Default, Debug, PartialEq, Serialize, Deserialize)]
pub struct ChatCompletionRequest {
    #[serde(rename = "model")]
    pub model: models::ModelIdentifier,
    /// A list of messages comprising the conversation so far
    #[serde(rename = "messages")]
    pub messages: Vec<models::ChatMessage>,
    /// The maximum number of tokens that can be generated in the completion
    #[serde(rename = "max_tokens", skip_serializing_if = "Option::is_none")]
    pub max_tokens: Option<u32>,
    /// Sampling temperature to use
    #[serde(rename = "temperature", skip_serializing_if = "Option::is_none")]
    pub temperature: Option<f64>,
    /// Nucleus sampling parameter
    #[serde(rename = "top_p", skip_serializing_if = "Option::is_none")]
    pub top_p: Option<f64>,
    /// If set, partial message deltas will be sent
    #[serde(rename = "stream", skip_serializing_if = "Option::is_none")]
    pub stream: Option<bool>,
    #[serde(rename = "stop", skip_serializing_if = "Option::is_none")]
    pub stop: Option<models::StopCondition>,
    #[serde(rename = "response_format", skip_serializing_if = "Option::is_none")]
    pub response_format: Option<models::ResponseFormat>,
    #[serde(rename = "tools", skip_serializing_if = "Option::is_none")]
    pub tools: Option<Vec<models::Tool>>,
    #[serde(rename = "tool_choice", skip_serializing_if = "Option::is_none")]
    pub tool_choice: Option<models::ToolChoiceOption>,
}

impl ChatCompletionRequest {
    pub fn new(
        model: models::ModelIdentifier,
        messages: Vec<models::ChatMessage>,
    ) -> ChatCompletionRequest {
        ChatCompletionRequest {
            model,
            messages,
            max_tokens: None,
            temperature: None,
            top_p: None,
            stream: None,
            stop: None,
            response_format: None,
            tools: None,
            tool_choice: None,
        }
    }
}
