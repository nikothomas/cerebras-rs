/*
 * Cerebras Inference API
 *
 * The Cerebras Inference API offers developers a low-latency solution for AI model inference  powered by Cerebras Wafer-Scale Engines and CS-3 systems. The API provides access to  high-performance language models with unprecedented speed for AI inference workloads. 
 *
 * The version of the OpenAPI document: 1.0.0
 * Contact: support@cerebras.ai
 * Generated by: https://openapi-generator.tech
 */

use crate::models;
use serde::{Deserialize, Serialize};

#[derive(Clone, Default, Debug, PartialEq, Serialize, Deserialize)]
pub struct CreateChatCompletionResponse {
    #[serde(rename = "id", skip_serializing_if = "Option::is_none")]
    pub id: Option<String>,
    #[serde(rename = "object", skip_serializing_if = "Option::is_none")]
    pub object: Option<String>,
    #[serde(rename = "created", skip_serializing_if = "Option::is_none")]
    pub created: Option<i32>,
    #[serde(rename = "model", skip_serializing_if = "Option::is_none")]
    pub model: Option<String>,
    #[serde(rename = "choices", skip_serializing_if = "Option::is_none")]
    pub choices: Option<Vec<models::ChatChoice>>,
    #[serde(rename = "usage", skip_serializing_if = "Option::is_none")]
    pub usage: Option<models::Usage>,
    #[serde(rename = "time_info", skip_serializing_if = "Option::is_none")]
    pub time_info: Option<models::TimeInfo>,
}

impl CreateChatCompletionResponse {
    pub fn new() -> CreateChatCompletionResponse {
        CreateChatCompletionResponse {
            id: None,
            object: None,
            created: None,
            model: None,
            choices: None,
            usage: None,
            time_info: None,
        }
    }
}

