/*
 * Cerebras Inference API
 *
 * The Cerebras Inference API offers developers a low-latency solution for AI model inference  powered by Cerebras Wafer-Scale Engines and CS-3 systems. The API provides access to  high-performance language models with unprecedented speed for AI inference workloads. 
 *
 * The version of the OpenAPI document: 1.0.0
 * Contact: support@cerebras.ai
 * Generated by: https://openapi-generator.tech
 */

use crate::models;
use serde::{Deserialize, Serialize};

/// 
#[derive(Clone, Copy, Debug, Eq, PartialEq, Ord, PartialOrd, Hash, Serialize, Deserialize)]
pub enum ModelIdentifier {
    #[serde(rename = "llama-4-scout-17b-16e-instruct")]
    Llama4Scout17b16eInstruct,
    #[serde(rename = "llama3.1-8b")]
    Llama3Period18b,
    #[serde(rename = "llama-3.3-70b")]
    Llama3Period370b,
    #[serde(rename = "qwen-3-32b")]
    Qwen332b,
    #[serde(rename = "deepseek-r1-distill-llama-70b")]
    DeepseekR1DistillLlama70b,

}

impl std::fmt::Display for ModelIdentifier {
    fn fmt(&self, f: &mut std::fmt::Formatter) -> std::fmt::Result {
        match self {
            Self::Llama4Scout17b16eInstruct => write!(f, "llama-4-scout-17b-16e-instruct"),
            Self::Llama3Period18b => write!(f, "llama3.1-8b"),
            Self::Llama3Period370b => write!(f, "llama-3.3-70b"),
            Self::Qwen332b => write!(f, "qwen-3-32b"),
            Self::DeepseekR1DistillLlama70b => write!(f, "deepseek-r1-distill-llama-70b"),
        }
    }
}

impl Default for ModelIdentifier {
    fn default() -> ModelIdentifier {
        Self::Llama4Scout17b16eInstruct
    }
}

